{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a0b0fd5-9693-4e71-8246-a2d04c3cea77",
   "metadata": {},
   "source": [
    "## Standard Algorithms Classifiers without dMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc861576-23c3-457e-9a10-5b019689feef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import random\n",
    "import math\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay, confusion_matrix, classification_report\n",
    "\n",
    "# Ignore the FutureWarning\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# Ignore the ConvergenceWarning and UserWarning\n",
    "warnings.simplefilter(action='ignore', category=ConvergenceWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac68afd5-ea27-430c-9999-9b9fb06d9504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe visualization: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>1062</th>\n",
       "      <th>1063</th>\n",
       "      <th>1064</th>\n",
       "      <th>1065</th>\n",
       "      <th>1066</th>\n",
       "      <th>1067</th>\n",
       "      <th>1068</th>\n",
       "      <th>1069</th>\n",
       "      <th>1070</th>\n",
       "      <th>1071</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.00</td>\n",
       "      <td>18.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>167.90</td>\n",
       "      <td>10.00</td>\n",
       "      <td>69.30</td>\n",
       "      <td>71.80</td>\n",
       "      <td>10.00</td>\n",
       "      <td>12.20</td>\n",
       "      <td>21.30</td>\n",
       "      <td>...</td>\n",
       "      <td>14.30</td>\n",
       "      <td>126.00</td>\n",
       "      <td>14.40</td>\n",
       "      <td>10.00</td>\n",
       "      <td>18.60</td>\n",
       "      <td>115.60</td>\n",
       "      <td>21.50</td>\n",
       "      <td>129.80</td>\n",
       "      <td>406.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>195.08</td>\n",
       "      <td>24.80</td>\n",
       "      <td>35.03</td>\n",
       "      <td>148.51</td>\n",
       "      <td>201.51</td>\n",
       "      <td>383.86</td>\n",
       "      <td>126.60</td>\n",
       "      <td>1445.40</td>\n",
       "      <td>116.10</td>\n",
       "      <td>491.04</td>\n",
       "      <td>...</td>\n",
       "      <td>99.97</td>\n",
       "      <td>58.25</td>\n",
       "      <td>236.01</td>\n",
       "      <td>141.16</td>\n",
       "      <td>58.38</td>\n",
       "      <td>173.17</td>\n",
       "      <td>19.15</td>\n",
       "      <td>85.01</td>\n",
       "      <td>330.86</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200.60</td>\n",
       "      <td>144.75</td>\n",
       "      <td>10.00</td>\n",
       "      <td>354.78</td>\n",
       "      <td>30.22</td>\n",
       "      <td>65.74</td>\n",
       "      <td>22.32</td>\n",
       "      <td>160.32</td>\n",
       "      <td>22.55</td>\n",
       "      <td>504.75</td>\n",
       "      <td>...</td>\n",
       "      <td>50.63</td>\n",
       "      <td>229.28</td>\n",
       "      <td>24.16</td>\n",
       "      <td>83.77</td>\n",
       "      <td>77.02</td>\n",
       "      <td>10.00</td>\n",
       "      <td>72.72</td>\n",
       "      <td>10.00</td>\n",
       "      <td>17.87</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>133.38</td>\n",
       "      <td>178.00</td>\n",
       "      <td>43.22</td>\n",
       "      <td>158.48</td>\n",
       "      <td>99.18</td>\n",
       "      <td>32.91</td>\n",
       "      <td>20.36</td>\n",
       "      <td>41.74</td>\n",
       "      <td>10.00</td>\n",
       "      <td>205.24</td>\n",
       "      <td>...</td>\n",
       "      <td>44.99</td>\n",
       "      <td>82.91</td>\n",
       "      <td>10.00</td>\n",
       "      <td>69.53</td>\n",
       "      <td>76.13</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>68.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50.82</td>\n",
       "      <td>14.08</td>\n",
       "      <td>20.20</td>\n",
       "      <td>75.30</td>\n",
       "      <td>177.54</td>\n",
       "      <td>257.13</td>\n",
       "      <td>122.24</td>\n",
       "      <td>863.44</td>\n",
       "      <td>178.97</td>\n",
       "      <td>116.32</td>\n",
       "      <td>...</td>\n",
       "      <td>210.40</td>\n",
       "      <td>47.75</td>\n",
       "      <td>60.41</td>\n",
       "      <td>209.18</td>\n",
       "      <td>39.18</td>\n",
       "      <td>10.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>25.71</td>\n",
       "      <td>70.41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1121.80</td>\n",
       "      <td>224.44</td>\n",
       "      <td>66.62</td>\n",
       "      <td>410.88</td>\n",
       "      <td>52.57</td>\n",
       "      <td>18.63</td>\n",
       "      <td>10.00</td>\n",
       "      <td>13.43</td>\n",
       "      <td>30.61</td>\n",
       "      <td>551.52</td>\n",
       "      <td>...</td>\n",
       "      <td>107.01</td>\n",
       "      <td>357.16</td>\n",
       "      <td>51.32</td>\n",
       "      <td>100.35</td>\n",
       "      <td>141.57</td>\n",
       "      <td>10.00</td>\n",
       "      <td>86.09</td>\n",
       "      <td>10.00</td>\n",
       "      <td>32.27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>684.12</td>\n",
       "      <td>278.79</td>\n",
       "      <td>19.86</td>\n",
       "      <td>669.72</td>\n",
       "      <td>292.81</td>\n",
       "      <td>150.38</td>\n",
       "      <td>22.19</td>\n",
       "      <td>101.70</td>\n",
       "      <td>19.62</td>\n",
       "      <td>459.92</td>\n",
       "      <td>...</td>\n",
       "      <td>49.76</td>\n",
       "      <td>442.09</td>\n",
       "      <td>51.79</td>\n",
       "      <td>379.56</td>\n",
       "      <td>16.12</td>\n",
       "      <td>10.00</td>\n",
       "      <td>94.07</td>\n",
       "      <td>27.33</td>\n",
       "      <td>125.53</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>749.21</td>\n",
       "      <td>146.43</td>\n",
       "      <td>34.77</td>\n",
       "      <td>455.08</td>\n",
       "      <td>17.64</td>\n",
       "      <td>26.67</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>323.59</td>\n",
       "      <td>...</td>\n",
       "      <td>13.50</td>\n",
       "      <td>565.39</td>\n",
       "      <td>45.58</td>\n",
       "      <td>97.57</td>\n",
       "      <td>66.09</td>\n",
       "      <td>10.00</td>\n",
       "      <td>31.06</td>\n",
       "      <td>10.00</td>\n",
       "      <td>89.46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>70.09</td>\n",
       "      <td>194.13</td>\n",
       "      <td>33.55</td>\n",
       "      <td>322.62</td>\n",
       "      <td>57.08</td>\n",
       "      <td>26.94</td>\n",
       "      <td>10.00</td>\n",
       "      <td>174.85</td>\n",
       "      <td>31.95</td>\n",
       "      <td>154.11</td>\n",
       "      <td>...</td>\n",
       "      <td>35.85</td>\n",
       "      <td>393.48</td>\n",
       "      <td>71.97</td>\n",
       "      <td>113.46</td>\n",
       "      <td>55.62</td>\n",
       "      <td>10.00</td>\n",
       "      <td>34.94</td>\n",
       "      <td>10.00</td>\n",
       "      <td>103.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>72.31</td>\n",
       "      <td>152.57</td>\n",
       "      <td>34.64</td>\n",
       "      <td>192.65</td>\n",
       "      <td>41.34</td>\n",
       "      <td>199.35</td>\n",
       "      <td>83.93</td>\n",
       "      <td>349.10</td>\n",
       "      <td>10.00</td>\n",
       "      <td>17.16</td>\n",
       "      <td>...</td>\n",
       "      <td>104.75</td>\n",
       "      <td>52.01</td>\n",
       "      <td>10.00</td>\n",
       "      <td>32.55</td>\n",
       "      <td>60.38</td>\n",
       "      <td>10.00</td>\n",
       "      <td>24.70</td>\n",
       "      <td>10.00</td>\n",
       "      <td>42.91</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>142.00</td>\n",
       "      <td>148.75</td>\n",
       "      <td>47.02</td>\n",
       "      <td>398.98</td>\n",
       "      <td>176.63</td>\n",
       "      <td>151.78</td>\n",
       "      <td>15.15</td>\n",
       "      <td>11.86</td>\n",
       "      <td>10.00</td>\n",
       "      <td>190.23</td>\n",
       "      <td>...</td>\n",
       "      <td>27.01</td>\n",
       "      <td>183.47</td>\n",
       "      <td>20.61</td>\n",
       "      <td>141.05</td>\n",
       "      <td>158.62</td>\n",
       "      <td>10.00</td>\n",
       "      <td>123.04</td>\n",
       "      <td>10.00</td>\n",
       "      <td>88.14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>62.39</td>\n",
       "      <td>343.81</td>\n",
       "      <td>13.90</td>\n",
       "      <td>267.63</td>\n",
       "      <td>67.20</td>\n",
       "      <td>29.99</td>\n",
       "      <td>66.57</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>309.74</td>\n",
       "      <td>...</td>\n",
       "      <td>31.77</td>\n",
       "      <td>76.91</td>\n",
       "      <td>33.13</td>\n",
       "      <td>10.87</td>\n",
       "      <td>24.56</td>\n",
       "      <td>10.00</td>\n",
       "      <td>19.33</td>\n",
       "      <td>10.00</td>\n",
       "      <td>55.28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>142.01</td>\n",
       "      <td>157.37</td>\n",
       "      <td>345.22</td>\n",
       "      <td>557.40</td>\n",
       "      <td>387.03</td>\n",
       "      <td>80.37</td>\n",
       "      <td>41.70</td>\n",
       "      <td>236.50</td>\n",
       "      <td>10.00</td>\n",
       "      <td>152.66</td>\n",
       "      <td>...</td>\n",
       "      <td>41.02</td>\n",
       "      <td>450.13</td>\n",
       "      <td>137.08</td>\n",
       "      <td>221.03</td>\n",
       "      <td>36.09</td>\n",
       "      <td>10.00</td>\n",
       "      <td>51.00</td>\n",
       "      <td>12.55</td>\n",
       "      <td>32.06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>297.88</td>\n",
       "      <td>24.09</td>\n",
       "      <td>67.88</td>\n",
       "      <td>261.75</td>\n",
       "      <td>91.16</td>\n",
       "      <td>36.95</td>\n",
       "      <td>61.20</td>\n",
       "      <td>338.90</td>\n",
       "      <td>10.00</td>\n",
       "      <td>709.87</td>\n",
       "      <td>...</td>\n",
       "      <td>164.57</td>\n",
       "      <td>264.84</td>\n",
       "      <td>92.30</td>\n",
       "      <td>195.50</td>\n",
       "      <td>56.81</td>\n",
       "      <td>10.00</td>\n",
       "      <td>31.09</td>\n",
       "      <td>81.71</td>\n",
       "      <td>132.18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>83.10</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>44.10</td>\n",
       "      <td>87.00</td>\n",
       "      <td>53.30</td>\n",
       "      <td>48.70</td>\n",
       "      <td>66.60</td>\n",
       "      <td>10.00</td>\n",
       "      <td>31.00</td>\n",
       "      <td>...</td>\n",
       "      <td>21.60</td>\n",
       "      <td>44.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>20.50</td>\n",
       "      <td>19.90</td>\n",
       "      <td>44.70</td>\n",
       "      <td>10.00</td>\n",
       "      <td>82.60</td>\n",
       "      <td>217.60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>100.04</td>\n",
       "      <td>202.18</td>\n",
       "      <td>32.46</td>\n",
       "      <td>489.38</td>\n",
       "      <td>12.26</td>\n",
       "      <td>18.27</td>\n",
       "      <td>13.54</td>\n",
       "      <td>85.88</td>\n",
       "      <td>10.00</td>\n",
       "      <td>211.97</td>\n",
       "      <td>...</td>\n",
       "      <td>10.00</td>\n",
       "      <td>392.25</td>\n",
       "      <td>95.09</td>\n",
       "      <td>75.69</td>\n",
       "      <td>56.91</td>\n",
       "      <td>164.52</td>\n",
       "      <td>18.38</td>\n",
       "      <td>113.50</td>\n",
       "      <td>179.99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>49.74</td>\n",
       "      <td>35.55</td>\n",
       "      <td>25.80</td>\n",
       "      <td>68.36</td>\n",
       "      <td>188.23</td>\n",
       "      <td>128.74</td>\n",
       "      <td>39.74</td>\n",
       "      <td>426.75</td>\n",
       "      <td>91.66</td>\n",
       "      <td>194.92</td>\n",
       "      <td>...</td>\n",
       "      <td>286.57</td>\n",
       "      <td>14.59</td>\n",
       "      <td>48.77</td>\n",
       "      <td>223.78</td>\n",
       "      <td>23.38</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.40</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>130.26</td>\n",
       "      <td>98.77</td>\n",
       "      <td>209.99</td>\n",
       "      <td>107.41</td>\n",
       "      <td>49.06</td>\n",
       "      <td>57.54</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.39</td>\n",
       "      <td>39.27</td>\n",
       "      <td>102.08</td>\n",
       "      <td>...</td>\n",
       "      <td>28.50</td>\n",
       "      <td>94.63</td>\n",
       "      <td>10.00</td>\n",
       "      <td>55.69</td>\n",
       "      <td>38.40</td>\n",
       "      <td>10.00</td>\n",
       "      <td>23.01</td>\n",
       "      <td>10.00</td>\n",
       "      <td>20.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>35.67</td>\n",
       "      <td>64.44</td>\n",
       "      <td>10.00</td>\n",
       "      <td>19.10</td>\n",
       "      <td>17.79</td>\n",
       "      <td>48.54</td>\n",
       "      <td>50.42</td>\n",
       "      <td>16.23</td>\n",
       "      <td>10.00</td>\n",
       "      <td>563.42</td>\n",
       "      <td>...</td>\n",
       "      <td>24.76</td>\n",
       "      <td>65.67</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>42.72</td>\n",
       "      <td>10.00</td>\n",
       "      <td>33.12</td>\n",
       "      <td>35.50</td>\n",
       "      <td>93.22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>44.49</td>\n",
       "      <td>60.64</td>\n",
       "      <td>40.65</td>\n",
       "      <td>81.96</td>\n",
       "      <td>120.91</td>\n",
       "      <td>11.95</td>\n",
       "      <td>20.95</td>\n",
       "      <td>10.00</td>\n",
       "      <td>12.91</td>\n",
       "      <td>41.68</td>\n",
       "      <td>...</td>\n",
       "      <td>47.14</td>\n",
       "      <td>47.58</td>\n",
       "      <td>10.00</td>\n",
       "      <td>168.20</td>\n",
       "      <td>49.58</td>\n",
       "      <td>10.00</td>\n",
       "      <td>26.93</td>\n",
       "      <td>10.00</td>\n",
       "      <td>12.76</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10.00</td>\n",
       "      <td>45.98</td>\n",
       "      <td>10.00</td>\n",
       "      <td>24.18</td>\n",
       "      <td>23.43</td>\n",
       "      <td>37.22</td>\n",
       "      <td>93.93</td>\n",
       "      <td>96.65</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>28.80</td>\n",
       "      <td>10.00</td>\n",
       "      <td>23.50</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>216.65</td>\n",
       "      <td>13.11</td>\n",
       "      <td>161.51</td>\n",
       "      <td>264.60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>134.11</td>\n",
       "      <td>140.79</td>\n",
       "      <td>83.01</td>\n",
       "      <td>303.56</td>\n",
       "      <td>37.22</td>\n",
       "      <td>29.21</td>\n",
       "      <td>17.26</td>\n",
       "      <td>60.12</td>\n",
       "      <td>10.00</td>\n",
       "      <td>80.13</td>\n",
       "      <td>...</td>\n",
       "      <td>10.00</td>\n",
       "      <td>174.35</td>\n",
       "      <td>45.74</td>\n",
       "      <td>140.47</td>\n",
       "      <td>37.68</td>\n",
       "      <td>10.00</td>\n",
       "      <td>22.62</td>\n",
       "      <td>10.00</td>\n",
       "      <td>41.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>131.43</td>\n",
       "      <td>52.31</td>\n",
       "      <td>14.13</td>\n",
       "      <td>264.75</td>\n",
       "      <td>41.89</td>\n",
       "      <td>49.23</td>\n",
       "      <td>23.17</td>\n",
       "      <td>94.51</td>\n",
       "      <td>10.00</td>\n",
       "      <td>282.83</td>\n",
       "      <td>...</td>\n",
       "      <td>14.00</td>\n",
       "      <td>153.72</td>\n",
       "      <td>39.75</td>\n",
       "      <td>70.90</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>62.11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>80.12</td>\n",
       "      <td>58.59</td>\n",
       "      <td>29.67</td>\n",
       "      <td>392.30</td>\n",
       "      <td>65.70</td>\n",
       "      <td>79.06</td>\n",
       "      <td>22.03</td>\n",
       "      <td>88.92</td>\n",
       "      <td>12.64</td>\n",
       "      <td>141.35</td>\n",
       "      <td>...</td>\n",
       "      <td>11.12</td>\n",
       "      <td>70.99</td>\n",
       "      <td>10.00</td>\n",
       "      <td>50.25</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>28.18</td>\n",
       "      <td>14.09</td>\n",
       "      <td>69.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>40.48</td>\n",
       "      <td>28.49</td>\n",
       "      <td>22.15</td>\n",
       "      <td>79.87</td>\n",
       "      <td>29.63</td>\n",
       "      <td>68.10</td>\n",
       "      <td>36.63</td>\n",
       "      <td>115.54</td>\n",
       "      <td>10.00</td>\n",
       "      <td>153.79</td>\n",
       "      <td>...</td>\n",
       "      <td>25.91</td>\n",
       "      <td>84.95</td>\n",
       "      <td>22.89</td>\n",
       "      <td>69.54</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>32.60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>166.92</td>\n",
       "      <td>76.55</td>\n",
       "      <td>44.05</td>\n",
       "      <td>201.64</td>\n",
       "      <td>10.00</td>\n",
       "      <td>24.30</td>\n",
       "      <td>28.29</td>\n",
       "      <td>27.02</td>\n",
       "      <td>10.00</td>\n",
       "      <td>300.05</td>\n",
       "      <td>...</td>\n",
       "      <td>15.26</td>\n",
       "      <td>83.60</td>\n",
       "      <td>10.00</td>\n",
       "      <td>23.58</td>\n",
       "      <td>42.16</td>\n",
       "      <td>10.00</td>\n",
       "      <td>27.74</td>\n",
       "      <td>10.00</td>\n",
       "      <td>23.63</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>148.67</td>\n",
       "      <td>45.35</td>\n",
       "      <td>77.62</td>\n",
       "      <td>332.68</td>\n",
       "      <td>12.62</td>\n",
       "      <td>17.21</td>\n",
       "      <td>13.08</td>\n",
       "      <td>77.39</td>\n",
       "      <td>10.00</td>\n",
       "      <td>441.17</td>\n",
       "      <td>...</td>\n",
       "      <td>30.64</td>\n",
       "      <td>218.44</td>\n",
       "      <td>41.80</td>\n",
       "      <td>64.77</td>\n",
       "      <td>16.05</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>60.58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>189.07</td>\n",
       "      <td>145.31</td>\n",
       "      <td>11.22</td>\n",
       "      <td>232.13</td>\n",
       "      <td>110.30</td>\n",
       "      <td>30.22</td>\n",
       "      <td>36.48</td>\n",
       "      <td>88.05</td>\n",
       "      <td>16.48</td>\n",
       "      <td>79.81</td>\n",
       "      <td>...</td>\n",
       "      <td>46.43</td>\n",
       "      <td>81.44</td>\n",
       "      <td>12.57</td>\n",
       "      <td>71.15</td>\n",
       "      <td>16.56</td>\n",
       "      <td>10.00</td>\n",
       "      <td>17.22</td>\n",
       "      <td>10.00</td>\n",
       "      <td>59.31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28 rows Ã— 1071 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          1       2       3       4       5       6       7        8       9  \\\n",
       "0     10.00   18.00   10.00  167.90   10.00   69.30   71.80    10.00   12.20   \n",
       "1    195.08   24.80   35.03  148.51  201.51  383.86  126.60  1445.40  116.10   \n",
       "2    200.60  144.75   10.00  354.78   30.22   65.74   22.32   160.32   22.55   \n",
       "3    133.38  178.00   43.22  158.48   99.18   32.91   20.36    41.74   10.00   \n",
       "4     50.82   14.08   20.20   75.30  177.54  257.13  122.24   863.44  178.97   \n",
       "5   1121.80  224.44   66.62  410.88   52.57   18.63   10.00    13.43   30.61   \n",
       "6    684.12  278.79   19.86  669.72  292.81  150.38   22.19   101.70   19.62   \n",
       "7    749.21  146.43   34.77  455.08   17.64   26.67   10.00    10.00   10.00   \n",
       "8     70.09  194.13   33.55  322.62   57.08   26.94   10.00   174.85   31.95   \n",
       "9     72.31  152.57   34.64  192.65   41.34  199.35   83.93   349.10   10.00   \n",
       "10   142.00  148.75   47.02  398.98  176.63  151.78   15.15    11.86   10.00   \n",
       "11    62.39  343.81   13.90  267.63   67.20   29.99   66.57    10.00   10.00   \n",
       "12   142.01  157.37  345.22  557.40  387.03   80.37   41.70   236.50   10.00   \n",
       "13   297.88   24.09   67.88  261.75   91.16   36.95   61.20   338.90   10.00   \n",
       "14    83.10   10.00   10.00   44.10   87.00   53.30   48.70    66.60   10.00   \n",
       "15   100.04  202.18   32.46  489.38   12.26   18.27   13.54    85.88   10.00   \n",
       "16    49.74   35.55   25.80   68.36  188.23  128.74   39.74   426.75   91.66   \n",
       "17   130.26   98.77  209.99  107.41   49.06   57.54   10.00    10.39   39.27   \n",
       "18    35.67   64.44   10.00   19.10   17.79   48.54   50.42    16.23   10.00   \n",
       "19    44.49   60.64   40.65   81.96  120.91   11.95   20.95    10.00   12.91   \n",
       "20    10.00   45.98   10.00   24.18   23.43   37.22   93.93    96.65   10.00   \n",
       "21   134.11  140.79   83.01  303.56   37.22   29.21   17.26    60.12   10.00   \n",
       "22   131.43   52.31   14.13  264.75   41.89   49.23   23.17    94.51   10.00   \n",
       "23    80.12   58.59   29.67  392.30   65.70   79.06   22.03    88.92   12.64   \n",
       "24    40.48   28.49   22.15   79.87   29.63   68.10   36.63   115.54   10.00   \n",
       "25   166.92   76.55   44.05  201.64   10.00   24.30   28.29    27.02   10.00   \n",
       "26   148.67   45.35   77.62  332.68   12.62   17.21   13.08    77.39   10.00   \n",
       "27   189.07  145.31   11.22  232.13  110.30   30.22   36.48    88.05   16.48   \n",
       "\n",
       "        10  ...    1062    1063    1064    1065    1066    1067    1068  \\\n",
       "0    21.30  ...   14.30  126.00   14.40   10.00   18.60  115.60   21.50   \n",
       "1   491.04  ...   99.97   58.25  236.01  141.16   58.38  173.17   19.15   \n",
       "2   504.75  ...   50.63  229.28   24.16   83.77   77.02   10.00   72.72   \n",
       "3   205.24  ...   44.99   82.91   10.00   69.53   76.13   10.00   10.00   \n",
       "4   116.32  ...  210.40   47.75   60.41  209.18   39.18   10.00   40.00   \n",
       "5   551.52  ...  107.01  357.16   51.32  100.35  141.57   10.00   86.09   \n",
       "6   459.92  ...   49.76  442.09   51.79  379.56   16.12   10.00   94.07   \n",
       "7   323.59  ...   13.50  565.39   45.58   97.57   66.09   10.00   31.06   \n",
       "8   154.11  ...   35.85  393.48   71.97  113.46   55.62   10.00   34.94   \n",
       "9    17.16  ...  104.75   52.01   10.00   32.55   60.38   10.00   24.70   \n",
       "10  190.23  ...   27.01  183.47   20.61  141.05  158.62   10.00  123.04   \n",
       "11  309.74  ...   31.77   76.91   33.13   10.87   24.56   10.00   19.33   \n",
       "12  152.66  ...   41.02  450.13  137.08  221.03   36.09   10.00   51.00   \n",
       "13  709.87  ...  164.57  264.84   92.30  195.50   56.81   10.00   31.09   \n",
       "14   31.00  ...   21.60   44.00   10.00   20.50   19.90   44.70   10.00   \n",
       "15  211.97  ...   10.00  392.25   95.09   75.69   56.91  164.52   18.38   \n",
       "16  194.92  ...  286.57   14.59   48.77  223.78   23.38   10.00   10.40   \n",
       "17  102.08  ...   28.50   94.63   10.00   55.69   38.40   10.00   23.01   \n",
       "18  563.42  ...   24.76   65.67   10.00   10.00   42.72   10.00   33.12   \n",
       "19   41.68  ...   47.14   47.58   10.00  168.20   49.58   10.00   26.93   \n",
       "20   10.00  ...   28.80   10.00   23.50   10.00   10.00  216.65   13.11   \n",
       "21   80.13  ...   10.00  174.35   45.74  140.47   37.68   10.00   22.62   \n",
       "22  282.83  ...   14.00  153.72   39.75   70.90   10.00   10.00   10.00   \n",
       "23  141.35  ...   11.12   70.99   10.00   50.25   10.00   10.00   28.18   \n",
       "24  153.79  ...   25.91   84.95   22.89   69.54   10.00   10.00   10.00   \n",
       "25  300.05  ...   15.26   83.60   10.00   23.58   42.16   10.00   27.74   \n",
       "26  441.17  ...   30.64  218.44   41.80   64.77   16.05   10.00   10.00   \n",
       "27   79.81  ...   46.43   81.44   12.57   71.15   16.56   10.00   17.22   \n",
       "\n",
       "      1069    1070  1071  \n",
       "0   129.80  406.80     0  \n",
       "1    85.01  330.86     0  \n",
       "2    10.00   17.87     0  \n",
       "3    10.00   68.88     0  \n",
       "4    25.71   70.41     0  \n",
       "5    10.00   32.27     0  \n",
       "6    27.33  125.53     0  \n",
       "7    10.00   89.46     0  \n",
       "8    10.00  103.43     0  \n",
       "9    10.00   42.91     0  \n",
       "10   10.00   88.14     0  \n",
       "11   10.00   55.28     0  \n",
       "12   12.55   32.06     0  \n",
       "13   81.71  132.18     0  \n",
       "14   82.60  217.60     1  \n",
       "15  113.50  179.99     1  \n",
       "16   10.00   10.00     1  \n",
       "17   10.00   20.67     1  \n",
       "18   35.50   93.22     1  \n",
       "19   10.00   12.76     1  \n",
       "20  161.51  264.60     1  \n",
       "21   10.00   41.57     1  \n",
       "22   10.00   62.11     1  \n",
       "23   14.09   69.67     1  \n",
       "24   10.00   32.60     1  \n",
       "25   10.00   23.63     1  \n",
       "26   10.00   60.58     1  \n",
       "27   10.00   59.31     1  \n",
       "\n",
       "[28 rows x 1071 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Dataset\n",
    "url = \"https://drive.google.com/file/d/16YkA1qJ4FHcBIvXZc17ifKSUzb_Xihth/view?usp=sharing\"\n",
    "url = \"https://drive.google.com/uc?id=\" + url.split('/')[-2]\n",
    "\n",
    "# Dataset visualization as DataFrame\n",
    "dataset = pd.read_csv(url, header = 0)\n",
    "print(\"Dataframe visualization: \")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9137d1ec-103c-415a-a404-05ae95192543",
   "metadata": {},
   "source": [
    "## KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05a4095b-dce7-4938-a644-3cf0f6d3a5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Clase 0       1.00      0.36      0.53        14\n",
      "     Clase 1       0.61      1.00      0.76        14\n",
      "\n",
      "    accuracy                           0.68        28\n",
      "   macro avg       0.80      0.68      0.64        28\n",
      "weighted avg       0.80      0.68      0.64        28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def knn_classifier_with_kfcv(dataset, k_neighbors, num_folds, output_file):\n",
    "    # Assuming the last column contains the class labels\n",
    "    X = dataset.iloc[:, :-1]\n",
    "    y = dataset.iloc[:, -1]\n",
    "\n",
    "    # Initialize kNN classifier\n",
    "    knn_classifier = KNeighborsClassifier(n_neighbors = k_neighbors)\n",
    "\n",
    "    # Initialize k-Fold Cross-Validation\n",
    "    kf = KFold(n_splits = num_folds, shuffle = True, random_state = 42)\n",
    "\n",
    "    # Results dictionary to store metrics for each iteration\n",
    "    results = {'Accuracy': [], 'Relevance_Indices': []}\n",
    "    # Initialize an empty list to accumulate indices\n",
    "    accumulated_indices = []\n",
    "    \n",
    "    # Initialize variables to keep track of the best accuracy and its corresponding indices\n",
    "    best_accuracy = 0.0\n",
    "    best_relevance_indices = None\n",
    "    best_classification_report = None\n",
    "\n",
    "    # Lists to store the predicted and actual labels\n",
    "    predicted_labels = []\n",
    "    actual_labels = []\n",
    "\n",
    "   # Iterate through each split in LOOCV\n",
    "    for (train_index, test_index) in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Fit the algorithm classifier on the training data\n",
    "        knn_classifier.fit(X_train, y_train)     # -- NaiveBayes\n",
    "\n",
    "        # Make predictions on the test data\n",
    "        y_pred = knn_classifier.predict(X_test)    # -- NB\n",
    "\n",
    "        # Store the predicted and actual labels\n",
    "        predicted_labels.extend(y_pred)\n",
    "        actual_labels.extend(y_test)\n",
    "\n",
    "    # Calculate accuracy and confusion matrix\n",
    "    labels = ['Clase 0', 'Clase 1']\n",
    "    #labels = ['Clase 0', 'Clase 1', 'Clase 2', 'Clase 3']\n",
    "    print(classification_report(actual_labels, predicted_labels, target_names = labels))\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run kNN classifier with kFCV and save the best classification report to a file\n",
    "results = knn_classifier_with_kfcv(dataset, k_neighbors = 1, num_folds = 10, output_file = 'Normal_best_classification_report_1nn.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b43dc1d6-957e-43d8-a600-67ae8aa6d2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Clase 0       1.00      0.21      0.35        14\n",
      "     Clase 1       0.56      1.00      0.72        14\n",
      "\n",
      "    accuracy                           0.61        28\n",
      "   macro avg       0.78      0.61      0.54        28\n",
      "weighted avg       0.78      0.61      0.54        28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run kNN classifier with kFCV and save the best classification report to a file\n",
    "results = knn_classifier_with_kfcv(dataset, k_neighbors = 3, num_folds = 10, output_file = 'Normal_best_classification_report_3nn.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a88fa46-5a0b-4095-abd8-956bd9367893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Clase 0       1.00      0.29      0.44        14\n",
      "     Clase 1       0.58      1.00      0.74        14\n",
      "\n",
      "    accuracy                           0.64        28\n",
      "   macro avg       0.79      0.64      0.59        28\n",
      "weighted avg       0.79      0.64      0.59        28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run kNN classifier with kFCV and save the best classification report to a file\n",
    "results = knn_classifier_with_kfcv(dataset, k_neighbors = 5, num_folds = 10, output_file = 'Normal_best_classification_report_5nn.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239f772b-a7a0-4592-bd7c-c13dcf8e01a7",
   "metadata": {},
   "source": [
    "## SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "805f56ca-a542-441c-ab3e-1af1d62b84d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Clase 0       1.00      0.86      0.92        14\n",
      "     Clase 1       0.88      1.00      0.93        14\n",
      "\n",
      "    accuracy                           0.93        28\n",
      "   macro avg       0.94      0.93      0.93        28\n",
      "weighted avg       0.94      0.93      0.93        28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "def svm_classifier_with_kfcv(dataset, num_folds, output_file):\n",
    "    # Assuming the last column contains the class labels\n",
    "    X = dataset.iloc[:, :-1]\n",
    "    y = dataset.iloc[:, -1]\n",
    "\n",
    "    # Initialize SVM classifier\n",
    "    svm_classifier = SVC()\n",
    "\n",
    "    # Initialize k-Fold Cross-Validation\n",
    "    kf = KFold(n_splits = num_folds, shuffle = True, random_state = 42)\n",
    "\n",
    "    # Results dictionary to store metrics for each iteration\n",
    "    results = {'Accuracy': [], 'Relevance_Indices': []}\n",
    "    \n",
    "    # Initialize variables to keep track of the best accuracy and its corresponding indices\n",
    "    best_accuracy = 0.0\n",
    "    best_classification_report = None\n",
    "\n",
    "    # Lists to store the predicted and actual labels\n",
    "    predicted_labels = []\n",
    "    actual_labels = []\n",
    "\n",
    "    # Iterate through each fold\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "            \n",
    "        # Fit SVM classifier\n",
    "        svm_classifier.fit(X_train, y_train)\n",
    "        # Predict on test set\n",
    "        y_pred = svm_classifier.predict(X_test)\n",
    "        # Store the predicted and actual labels\n",
    "        predicted_labels.extend(y_pred)\n",
    "        actual_labels.extend(y_test)\n",
    "\n",
    "    # Calculate accuracy and confusion matrix\n",
    "    labels = ['Clase 0', 'Clase 1']\n",
    "    #labels = ['Clase 0', 'Clase 1', 'Clase 2', 'Clase 3']\n",
    "    print(classification_report(actual_labels, predicted_labels, target_names = labels))\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run kNN classifier with kFCV and save the best classification report to a file\n",
    "results = svm_classifier_with_kfcv(dataset, num_folds = 10, output_file = 'Normal_best_classification_report_svm.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f70a1b-8a3a-461e-a25f-7e80b25467e5",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd625404-6db0-4183-ad15-810cbce827ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Clase 0       0.50      0.57      0.53        14\n",
      "     Clase 1       0.50      0.43      0.46        14\n",
      "\n",
      "    accuracy                           0.50        28\n",
      "   macro avg       0.50      0.50      0.50        28\n",
      "weighted avg       0.50      0.50      0.50        28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def random_forest_classifier_with_kfcv(dataset, num_folds, output_file):\n",
    "    # Assuming the last column contains the class labels\n",
    "    X = dataset.iloc[:, :-1]\n",
    "    y = dataset.iloc[:, -1]\n",
    "\n",
    "    # Initialize Random Forest classifier\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=10)\n",
    "\n",
    "    # Initialize k-Fold Cross-Validation\n",
    "    kf = KFold(n_splits = num_folds, shuffle = True, random_state = 42)\n",
    "\n",
    "    # Results dictionary to store metrics for each iteration\n",
    "    results = {'Accuracy': [], 'Relevance_Indices': []}\n",
    "    \n",
    "    # Initialize variables to keep track of the best accuracy and its corresponding indices\n",
    "    best_accuracy = 0.0\n",
    "    best_classification_report = None\n",
    "\n",
    "    # Lists to store the predicted and actual labels\n",
    "    predicted_labels = []\n",
    "    actual_labels = []\n",
    "\n",
    "    # Iterate through each fold\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "            \n",
    "        # Fit SVM classifier\n",
    "        rf_classifier.fit(X_train, y_train)\n",
    "        # Predict on test set\n",
    "        y_pred = rf_classifier.predict(X_test)\n",
    "        # Store the predicted and actual labels\n",
    "        predicted_labels.extend(y_pred)\n",
    "        actual_labels.extend(y_test)\n",
    "\n",
    "    # Calculate accuracy and confusion matrix\n",
    "    labels = ['Clase 0', 'Clase 1']\n",
    "    #labels = ['Clase 0', 'Clase 1', 'Clase 2', 'Clase 3']\n",
    "    print(classification_report(actual_labels, predicted_labels, target_names = labels))\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run SVM classifier with kFCV\n",
    "results = random_forest_classifier_with_kfcv(dataset, num_folds = 10, output_file = 'Normal_best_classification_report_rf.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7908c5a-5dee-403c-b8ed-1c9c7fed2b45",
   "metadata": {},
   "source": [
    "## Adaboost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc2cee71-eef8-4a16-9969-6522fc2659da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Clase 0       0.71      0.71      0.71        14\n",
      "     Clase 1       0.71      0.71      0.71        14\n",
      "\n",
      "    accuracy                           0.71        28\n",
      "   macro avg       0.71      0.71      0.71        28\n",
      "weighted avg       0.71      0.71      0.71        28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "def adaboost_classifier_with_kfcv(dataset, num_folds, output_file):\n",
    "    # Assuming the last column contains the class labels\n",
    "    X = dataset.iloc[:, :-1]\n",
    "    y = dataset.iloc[:, -1]\n",
    "\n",
    "    # Initialize AdaBoost classifier\n",
    "    adaboost_classifier = AdaBoostClassifier(n_estimators = 50, random_state = 42)\n",
    "\n",
    "    # Initialize k-Fold Cross-Validation\n",
    "    kf = KFold(n_splits = num_folds, shuffle = True, random_state = 42)\n",
    "\n",
    "    # Results dictionary to store metrics for each iteration\n",
    "    results = {'Accuracy': [], 'Relevance_Indices': []}\n",
    "    \n",
    "    # Initialize variables to keep track of the best accuracy and its corresponding indices\n",
    "    best_accuracy = 0.0\n",
    "    best_classification_report = None\n",
    "\n",
    "    # Lists to store the predicted and actual labels\n",
    "    predicted_labels = []\n",
    "    actual_labels = []\n",
    "\n",
    "    # Iterate through each fold\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "            \n",
    "        # Fit SVM classifier\n",
    "        adaboost_classifier.fit(X_train, y_train)\n",
    "        # Predict on test set\n",
    "        y_pred = adaboost_classifier.predict(X_test)\n",
    "        # Store the predicted and actual labels\n",
    "        predicted_labels.extend(y_pred)\n",
    "        actual_labels.extend(y_test)\n",
    "\n",
    "    # Calculate accuracy and confusion matrix\n",
    "    labels = ['Clase 0', 'Clase 1']\n",
    "    #labels = ['Clase 0', 'Clase 1', 'Clase 2', 'Clase 3']\n",
    "    print(classification_report(actual_labels, predicted_labels, target_names = labels))\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run MLP classifier with kFCV\n",
    "results = adaboost_classifier_with_kfcv(dataset, num_folds = 10, output_file = 'Normal_best_classification_report_adaboost.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c81734d-3758-44d7-9bc3-8da6b5643040",
   "metadata": {},
   "source": [
    "## MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a48a39da-02bc-4da2-89d0-9e2dc060b593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Clase 0       0.48      0.71      0.57        14\n",
      "     Clase 1       0.43      0.21      0.29        14\n",
      "\n",
      "    accuracy                           0.46        28\n",
      "   macro avg       0.45      0.46      0.43        28\n",
      "weighted avg       0.45      0.46      0.43        28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def mlp_classifier_with_kfcv(dataset, num_folds, output_file):\n",
    "    # Assuming the last column contains the class labels\n",
    "    X = dataset.iloc[:, :-1]\n",
    "    y = dataset.iloc[:, -1]\n",
    "\n",
    "    # Initialize MLP classifier with 10 hidden layers\n",
    "    mlp_classifier = MLPClassifier(hidden_layer_sizes = (20,), max_iter = 1000, early_stopping = True, random_state = 42)\n",
    "\n",
    "    # Initialize k-Fold Cross-Validation\n",
    "    kf = KFold(n_splits = num_folds, shuffle = True, random_state = 42)\n",
    "\n",
    "    # Results dictionary to store metrics for each iteration\n",
    "    results = {'Accuracy': [], 'Relevance_Indices': []}\n",
    "    \n",
    "    # Initialize variables to keep track of the best accuracy and its corresponding indices\n",
    "    best_accuracy = 0.0\n",
    "    best_classification_report = None\n",
    "\n",
    "    # Lists to store the predicted and actual labels\n",
    "    predicted_labels = []\n",
    "    actual_labels = []\n",
    "\n",
    "    # Iterate through each fold\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "            \n",
    "        # Fit SVM classifier\n",
    "        mlp_classifier.fit(X_train, y_train)\n",
    "        # Predict on test set\n",
    "        y_pred = mlp_classifier.predict(X_test)\n",
    "        # Store the predicted and actual labels\n",
    "        predicted_labels.extend(y_pred)\n",
    "        actual_labels.extend(y_test)\n",
    "\n",
    "    # Calculate accuracy and confusion matrix\n",
    "    labels = ['Clase 0', 'Clase 1']\n",
    "    #labels = ['Clase 0', 'Clase 1', 'Clase 2', 'Clase 3']\n",
    "    print(classification_report(actual_labels, predicted_labels, target_names = labels))\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run MLP classifier with kFCV\n",
    "results = mlp_classifier_with_kfcv(dataset, num_folds = 10, output_file = 'Normal_best_classification_report_mlp.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fc7d99-8d28-44e3-8a1e-b4cb9f432463",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6d112a5-f0a9-44d2-9037-7b09b9dfa995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Clase 0       0.59      0.93      0.72        14\n",
      "     Clase 1       0.83      0.36      0.50        14\n",
      "\n",
      "    accuracy                           0.64        28\n",
      "   macro avg       0.71      0.64      0.61        28\n",
      "weighted avg       0.71      0.64      0.61        28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "def naive_bayes_classifier_with_kfcv(dataset, num_folds, output_file):\n",
    "    # Assuming the last column contains the class labels\n",
    "    X = dataset.iloc[:, :-1]\n",
    "    y = dataset.iloc[:, -1]\n",
    "\n",
    "    # Initialize Gaussian Naive Bayes classifier\n",
    "    nb_classifier = GaussianNB()\n",
    "\n",
    "    # Initialize k-Fold Cross-Validation\n",
    "    kf = KFold(n_splits = num_folds, shuffle = True, random_state = 42)\n",
    "\n",
    "    # Results dictionary to store metrics for each iteration\n",
    "    results = {'Accuracy': [], 'Relevance_Indices': []}\n",
    "    \n",
    "    # Initialize variables to keep track of the best accuracy and its corresponding indices\n",
    "    best_accuracy = 0.0\n",
    "    best_classification_report = None\n",
    "\n",
    "    # Lists to store the predicted and actual labels\n",
    "    predicted_labels = []\n",
    "    actual_labels = []\n",
    "\n",
    "    # Iterate through each fold\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "            \n",
    "        # Fit SVM classifier\n",
    "        nb_classifier.fit(X_train, y_train)\n",
    "        # Predict on test set\n",
    "        y_pred = nb_classifier.predict(X_test)\n",
    "        # Store the predicted and actual labels\n",
    "        predicted_labels.extend(y_pred)\n",
    "        actual_labels.extend(y_test)\n",
    "\n",
    "    # Calculate accuracy and confusion matrix\n",
    "    labels = ['Clase 0', 'Clase 1']\n",
    "    #labels = ['Clase 0', 'Clase 1', 'Clase 2', 'Clase 3']\n",
    "    print(classification_report(actual_labels, predicted_labels, target_names = labels))\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run NB classifier with kFCV\n",
    "results = naive_bayes_classifier_with_kfcv(dataset, num_folds = 10, output_file = 'Normal_best_classification_report_nb.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
